# MultiHeadedAttention Class

Multi-head attention mechanism for Transformer.

## Usage

``` r
MultiHeadedAttention(h, d_model, dropout = 0.1)
```

## Arguments

- h:

  Number of attention heads

- d_model:

  Model dimensionality

- dropout:

  Dropout probability
