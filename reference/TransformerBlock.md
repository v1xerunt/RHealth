# TransformerBlock Class

Single Transformer encoder block with multi-head attention and FFN.

## Usage

``` r
TransformerBlock(hidden, attn_heads, dropout)
```

## Arguments

- hidden:

  Hidden size

- attn_heads:

  Number of attention heads

- dropout:

  Dropout rate
