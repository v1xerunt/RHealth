% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Metrics_multilabel.R
\name{multilabel_metrics_fn}
\alias{multilabel_metrics_fn}
\title{Multilabel Classification Metrics}
\usage{
multilabel_metrics_fn(y_true, y_prob, metrics = NULL, threshold = 0.5)
}
\arguments{
\item{y_true}{Binary matrix or data frame of ground‑truth labels with shape
(n_samples, n_labels), where 1 indicates presence and 0 indicates absence.}

\item{y_prob}{Numeric matrix of predicted probabilities with shape
(n_samples, n_labels). Will be converted to binary predictions using threshold.}

\item{metrics}{Character vector listing which metrics to compute.  Default
is \code{c("accuracy", "f1_micro", "f1_macro")}.}

\item{threshold}{Numeric threshold for converting probabilities to binary
predictions. Default is 0.5.}
}
\value{
Named numeric vector with one element per requested metric.
}
\description{
A configurable evaluator for \strong{multilabel classification} tasks, where each
sample can belong to multiple classes simultaneously. The function computes
various discrimination metrics commonly used for multilabel problems.

Supported \code{metrics} names:
\itemize{
\item \strong{"accuracy"} – Subset accuracy (exact match ratio)
\item \strong{"hamming_loss"} – Hamming loss (fraction of incorrect labels)
\item \strong{"f1_micro"} – F1 score, micro averaged across all labels
\item \strong{"f1_macro"} – F1 score, macro averaged across labels
\item \strong{"f1_weighted"} – F1 score, weighted averaged by label support
\item \strong{"f1_samples"} – F1 score, averaged across samples
\item \strong{"precision_micro"} – Precision, micro averaged
\item \strong{"precision_macro"} – Precision, macro averaged
\item \strong{"precision_weighted"} – Precision, weighted averaged
\item \strong{"precision_samples"} – Precision, averaged across samples
\item \strong{"recall_micro"} – Recall, micro averaged
\item \strong{"recall_macro"} – Recall, macro averaged
\item \strong{"recall_weighted"} – Recall, weighted averaged
\item \strong{"recall_samples"} – Recall, averaged across samples
\item \strong{"jaccard_micro"} – Jaccard index, micro averaged
\item \strong{"jaccard_macro"} – Jaccard index, macro averaged
\item \strong{"jaccard_weighted"} – Jaccard index, weighted averaged
\item \strong{"jaccard_samples"} – Jaccard index, averaged across samples
\item \strong{"roc_auc_micro"} – ROC AUC, micro averaged (requires y_prob)
\item \strong{"roc_auc_macro"} – ROC AUC, macro averaged (requires y_prob)
\item \strong{"roc_auc_weighted"} – ROC AUC, weighted averaged (requires y_prob)
\item \strong{"roc_auc_samples"} – ROC AUC, averaged across samples (requires y_prob)
\item \strong{"pr_auc_micro"} – PR AUC, micro averaged (requires y_prob)
\item \strong{"pr_auc_macro"} – PR AUC, macro averaged (requires y_prob)
\item \strong{"pr_auc_weighted"} – PR AUC, weighted averaged (requires y_prob)
\item \strong{"pr_auc_samples"} – PR AUC, averaged across samples (requires y_prob)
}
}
\examples{
set.seed(42)
n <- 100
k <- 5
y_true <- matrix(rbinom(n * k, 1, 0.3), nrow = n, ncol = k)
y_prob <- matrix(runif(n * k), nrow = n, ncol = k)
multilabel_metrics_fn(y_true, y_prob, metrics = c("accuracy", "f1_micro"))

}
