% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Model_RNNModel.R
\name{RNNLayer}
\alias{RNNLayer}
\title{RNNLayer Class}
\usage{
RNNLayer(
  input_size,
  hidden_size,
  rnn_type = "GRU",
  num_layers = 1,
  dropout = 0.5,
  bidirectional = FALSE
)
}
\arguments{
\item{input_size}{The number of expected features in the input \code{x}}

\item{hidden_size}{The number of features in the hidden state \code{h}}

\item{rnn_type}{Character, one of "GRU", "LSTM", or "RNN". Default "GRU".}

\item{num_layers}{Number of recurrent layers. E.g., setting \code{num_layers=2}
would mean stacking two GRUs together to form a \verb{stacked GRU},
with the second GRU taking in outputs of the first GRU and
computing the final results. Default: 1}

\item{dropout}{If non-zero, introduces a \code{Dropout} layer on the outputs of each
GRU layer except the last layer, with dropout probability equal to
\code{dropout}. Default: 0}

\item{bidirectional}{If \code{TRUE}, becomes a bidirectional GRU. Default: \code{FALSE}}
}
\description{
Recurrent neural network layer (GRU/LSTM/RNN) with built‑in
1‑based‑index safety, masking, dropout, bidirectionality and a learnable
fallback hidden vector for empty sequences.
}
\details{
\strong{Key design points}
\itemize{
\item Accepts \code{mask} indicating valid time steps (1‑based indexing respected).
\item Uses \code{pack_padded_sequence} + \code{pad_packed_sequence} for efficiency.
\item Samples whose sequence length is \emph{zero} (all‑zero rows) are skipped during
RNN computation \strong{and} later filled with a learnable parameter
\code{null_hidden} so downstream layers always receive a hidden vector.
\item Works for unidirectional or bidirectional networks. In the bidirectional
case, the last hidden state is built from the \strong{forward last step} and the
\strong{backward first step}, then projected back to \code{hidden_size}.
}
}
