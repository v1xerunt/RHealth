% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Metrics_multiclass.R
\name{multiclass_metrics_fn}
\alias{multiclass_metrics_fn}
\title{Multiclass Classification Metrics}
\usage{
multiclass_metrics_fn(y_true, y_prob, metrics = NULL, y_predset = NULL)
}
\arguments{
\item{y_true}{Numeric or integer vector of ground‑truth labels (1, 2, ..., K) using R's 1-based indexing.}

\item{y_prob}{Numeric matrix of predicted probabilities with shape (n_samples, n_classes).}

\item{metrics}{Character vector listing which metrics to compute.  Default
is \code{c("accuracy", "f1_macro", "f1_micro")}.}

\item{y_predset}{Optional matrix for prediction set metrics. Default is NULL.}
}
\value{
Named numeric vector with one element per requested metric.
}
\description{
A configurable evaluator for \strong{multiclass classification} tasks.  The function
reproduces the behaviour of Python's \code{multiclass_metrics_fn}, covering classical
discrimination metrics (ROC‑AUC with various averaging strategies, F1, accuracy, …)
as well as calibration metrics (ECE / adaptive ECE / classwise ECE).

Supported \code{metrics} names:
\itemize{
\item \strong{"roc_auc_macro_ovo"} – ROC AUC, macro averaged over one-vs-one
\item \strong{"roc_auc_macro_ovr"} – ROC AUC, macro averaged over one-vs-rest
\item \strong{"roc_auc_weighted_ovo"} – ROC AUC, weighted averaged over one-vs-one
\item \strong{"roc_auc_weighted_ovr"} – ROC AUC, weighted averaged over one-vs-rest
\item \strong{"accuracy"} – Overall accuracy
\item \strong{"balanced_accuracy"} – Balanced accuracy (useful for imbalanced datasets)
\item \strong{"f1_micro"} – F1 score, micro averaged
\item \strong{"f1_macro"} – F1 score, macro averaged
\item \strong{"f1_weighted"} – F1 score, weighted averaged
\item \strong{"jaccard_micro"} – Jaccard index, micro averaged
\item \strong{"jaccard_macro"} – Jaccard index, macro averaged
\item \strong{"jaccard_weighted"} – Jaccard index, weighted averaged
\item \strong{"cohen_kappa"} – Cohen's κ
\item \strong{"brier_top1"} – Brier score between top prediction and true label
\item \strong{"ECE"} – Expected Calibration Error (equal‑width bins)
\item \strong{"ECE_adapt"} – Adaptive ECE (equal‑size bins)
\item \strong{"cwECEt"} – Classwise ECE with threshold
\item \strong{"cwECEt_adapt"} – Classwise adaptive ECE with threshold
\item \strong{"hits@n"} – Computes HITS@1, HITS@5, HITS@10
\item \strong{"mean_rank"} – Computes mean rank and mean reciprocal rank
}
}
\examples{
set.seed(42)
n <- 100
k <- 4
y_true <- sample(1:k, n, replace = TRUE)
y_prob <- matrix(runif(n * k), nrow = n, ncol = k)
y_prob <- y_prob / rowSums(y_prob)  # normalize to sum to 1
multiclass_metrics_fn(y_true, y_prob, metrics = c("accuracy", "f1_macro"))

}
